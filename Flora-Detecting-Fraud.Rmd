---
title: "Fraud Detecting Flora"
author: "Kostas Voulgaropoulos"
date: "3 Sept 2019"
output: 
  pdf_document:
    keep_tex: yes
    toc: yes
    number_sections: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

This kernel aims to create a machine learning algorithm able to detect fraud in financial transactions. The underlying reasoning is that an algorithm trained to predict fraud correctly can efficiently detect and stop fraudulent transactions in real time.

For that, we employ a [publicly available dataset](https://www.kaggle.com/ntnu-testimon/paysim1) generated by the [PaySim](https://github.com/EdgarLopezPhD/PaySim) simulator by Dr. Edgar Lopez. For ease of reproduction and computation, we isolated the first 1 million rows of it. 

More specifically, it contains 10 predictors of numeric, integer and character type. 

Our target variable is categorical (i.e. label, not quantity) and binary (meaning the number of distinct classes are 2).

For starters we explore our data with an extensive Exploratory Data Analysis. With the insights the EDA equips us, we perform data reduction and engineering whenever possible. Because of the great imbalance in the frequency of the two classes (99% No_Fraud vs 1% Fraud), we undersample the prevalent class to reach a  60/40 ratio. Finally, although our algorithms do not require normalized data to work, we center and scale our numerical features for speed of computations. 

The algorithms employed for modelling are classification tree (rpart library) and random forest (rf library). These are chosen because they are good at handling different types of features (since we end up with factor, numerical and logical predictors) and relatively fast to train. On the other hand, their disadvantages include low interpretability (they are actually black boxes), great memory occupation and potential overfitting in case we don't tune their parameters fine. These barriers can be bypassed.  

Success of our algorithms is evaluated using the Area under the Receiver Operating Characteristics (ROC) curve. One could argue that financial institutions prefer to be more conservative in their anti-fraud policy (thus prefering an algorithm prone to classifying transactions as frauds) and desire to minimize the algorithm's False Negatives (since we consider No Fraud class as positive), measured by the Sensitivity metric. But on the other hand, institutions could also care about customer satisfaction and do not wish to unnecessarily block non fraudulent transactions classified as frauds. Under this scenario, they would seek to minimize False Positives (cases where the algo predicted Fraud even though it wasn't) and thus maximize specificity. A pretty convenient solution that takes into consideration both strategies is the ROC curve. This curve is constructed by using model's Specificity on the x-axis and Sensitivity on the y-axis.  

Because our algorithms are trained on the balanced train set, we also test it in the unbalanced test set, where fraud cases are very rare. Success rates are very close to those obtained from the balanced dataset.


```{r library_import, echo=TRUE,warning=FALSE,message=FALSE}
#Libraries
library(tidyverse) # data manipulation
library(gridExtra) # showing multiple plots
library(grid) # assisting gridExtra
library(caret) # model training 
library(GGally) # for pairplot
library(pROC) # ROC Curve
library(ROCR) # ROC Curve
```


```{r data_import, echo = FALSE}
zipfile <- paste0(getwd(),"/PaySim1M.zip") %>% 
  str_replace_all( "/", "\\\\" )

df <- unzip(zipfile = zipfile) %>% 
        read.csv(stringsAsFactors = FALSE, nrow = 8000) 

```


# EDA and Data Cleaning

Successfull machine learning requires good knowledge of the underlying data. This knowledge can help us:
* eliminate unnecessary observations/features, 
* transform the remaining and 
* create new ones. 

In this section we will explore our data with Exploratory Data Analysis and at the same time reconstruct them. Before we start, let's take a look at the data.

```{r str, echo = FALSE}
#A glance at the structure
df %>% str()
```

Before we dive into analyzing our features, let's quickly look at our target variable.

## Class Balance of Target Variable

For starters, we will look at the **frequency** and the **percentage** of each type of **target** variable (Fraud vs Not Fraud).

```{r fraud_dominance, echo=FALSE}
df %>% ggplot(aes(x = isFraud, y = (..count..))) +  
    geom_bar(fill =  c( "#5b7fb0", "#c2695b" ) , stat = "count")+
    geom_label(stat='count',aes(   label=  paste0(  round(   ((..count..)/sum(..count..)) ,4)*100 ,  "%" ) ) )+ #add the percentage of each type as label
    labs(x = "Fraud or Not", y = "Frequency", title = "Frequency of Fraud", subtitle = "Labels as percent of Total Observations")+
    theme_linedraw()
```

This great imbalance between the two target classes will lead our model to a bias towards the dominating variable. This means that since more than 99% of the cases are not frauds our algorithm will feel comfortable with predicting "Not Fraud". It has a 99% chance to be right.

Later, in the preprocessing section, we will undersample the majority class to create a more balanced dataset. We could also synthesize minority instances with a popular and more sophisticated technique called SMOTE but regarding low-dimensional data, simple undersampling [tends to outperform](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.79.4356&rep=rep1&type=pdf) it in most situations. In addition, the size of our dataset is sufficient to exempt us from the need to create artificial data.


## Factors

Looking at the dataset structure, we can observe that, apart from the six numeric features, we have three integer and three character columns. Integer columns get treated as factors when they don't express actual numeric value, but they just mask factor features. Because it seems that that's our case, we include the integer columns under the umbrella of factors.

As for each column separately, from this first impression:

* *isFlaggedFraud* looks like the prediction of the existing algorithm. We don't want our model to depend on someone else's prediction, thus we will get rid of it.

* *isFraud*, the target variable, will be turned into a factor and renamed for ease of plotting (the factor levels will become meaningful labels).

* *nameDest* and *nameOrig* look random and without predictive power, but we will explore them thoroughly in case we can extract any value from them.

* *type* is a potentially powerful feature. We can easily hypothesize that fraud is prone to some types of transactions. For example, cash in and payment (giving money) are quite improbable to co-exist with a fraud scheme. This will also get explored in detail.

* *step* is just the time variable. It will probably be of no use in our prediction as fraud can happen at any time. Still, we will treat it as predictive until otherwise proved.


For starters, we will take a look at the number of unique values the character columns contain. This will help us shape our cleaning strategy.


```{r distinct_values_per_factor, echo = FALSE }
#A quick glance to the number of dinstinct values of each character column

print("Number of Distinct Values in Character Vectors")
df[ , sapply(df,is.character)] %>% sapply(n_distinct) %>% print()

```

*type* contains only 5 distinct values. We will explore them thoroughtly soon.

*nameOrig* and *nameDest* (name of origin and name of destination respectively), on the other hand, contain way too many distinct values to be treated as factors. The numbers are obviously random, probably generated by a hash function. Nevertheless, the prefix might symbolize some implicit information (for example the type of account - be it a savings or checking account etc.). 



### Name of Origin / Destination

To test our assumption, let's look at the number of distinct prefixes and their frequency.

```{r distinct_prefixes, echo= FALSE}
print("Distinct Prefixes in nameOrig and nameDest columns : ")

df %>% mutate(origin_name_prefix = str_sub(nameOrig,1,1), #create columns containing prefix
              dest_name_prefix = str_sub(nameDest, 1, 1)) %>%
        select( c( origin_name_prefix,dest_name_prefix ) ) %>% #isolate them 
        table() %>% #count of distinct occurences
        print() #show
```

It appears there are only two prefixes, C and M, almost equally present in the *name_dest_pref* column. However, *name_orig_pref* contains only C's, thus it could have no predictive power.

As a result, we can 

* **add** the **destination prefix** column, 

* **omit** creating the respective feature from the **origin prefix** and 

* get **rid** of the **initial columns**. 

* On top of that, that's a great opportunity to get **rid** of the useless to us *isFlaggedFraud* column.

In addition, we will turn the remaining integer and character columns into **factors**.

Finally, to render our plots more readable, we will **rename** the **balance columns** and the **levels of the target variable**.


```{r as_factors, echo=FALSE }
df <- df %>% mutate(dest_name_prefix = str_sub(nameDest, 1, 1)) %>% #create columns containing prefix
                    select(-c('nameOrig','nameDest','isFlaggedFraud')) %>% #discard columns with old names
                    mutate_if(is.integer, as.factor) %>% #turn integer features into factors
                    mutate_if(is.character,as.factor) %>% #turn character features into factors
                    rename("Old_Balance_of_Origin" = "oldbalanceOrg" , #rename into a readable format
                            "New_Balance_of_Origin" = "newbalanceOrig",
                            "Old_Balance_of_Destination" = "oldbalanceDest",
                            "New_Balance_of_Destination" = "newbalanceDest")

levels(df$isFraud) <- c("No_Fraud", "Fraud" ) #rename into a readable format
```


### Destination Names

Before we eliminate these transactions types, we will approach in a similar manner the *dest_name_prefix column* to account for similar effects.


```{r dest_name_prefix_plot, echo=FALSE}
#Define colors associated with each group
dest_name_colors <- c("C" = "#6A4491", "M" = "#CC6600")

#Count of prefix in fraud cases

p3 <- df %>% filter(isFraud == "Fraud") %>% 
    ggplot( aes( x = isFraud, fill = dest_name_prefix ) ) +
    geom_bar()+
    scale_fill_manual(values=dest_name_colors) +
    labs(title = "Destination Name Prefix",subtitle = "Fraud Cases", x = "", y = "Frequency" )+
    theme_classic()

p4 <-df %>% filter(isFraud == "No_Fraud") %>% 
    ggplot( aes( x = isFraud, fill = dest_name_prefix ) ) +
    geom_bar()+
    scale_fill_manual(values=dest_name_colors) +
    labs(title = "Destination Name Prefix", subtitle =  "No Fraud Cases", x = "", y = "" )+
    theme_classic()

grid.arrange(p3,p4,ncol = 2)
rm(p3,p4)
```

Accordingly, fraud coincides only with the "C" prefix. We could remove observations containing the "M". 
After the removal, this column contains only "C"s (ie zero variance), meaning we can drop it.

This process became of use to us not as a new column that can predict the result, but as a signal that led us to discard a significant amount of useless rows.

### Transaction Type

Next, we will explore the presence of each transaction type in the fraud vs the not fraud cases.


```{r trans_type_plot, echo=FALSE }
#Create a color palette
type_colors <- c(CASH_IN = "#78858f", CASH_OUT = "#CC6600", DEBIT ="#6A4491", PAYMENT = "#FFC233", TRANSFER = "#788f78")

#Count of type in fraud cases
p1 <- df %>% filter(isFraud == "Fraud") %>% 
    ggplot( aes( x = isFraud, fill = type ) ) +
    geom_bar()+
    theme(axis.text.x = element_blank())+
    scale_fill_manual(values=type_colors) +
    labs(title = "Transaction Type", subtitle =  "Fraud Cases", x = "", y = "" ) + 
    coord_flip() + theme_linedraw()+
    theme(axis.text.y = element_blank())


p2 <-df %>% filter(isFraud == "No_Fraud") %>% 
    ggplot( aes( x = isFraud, fill = type ) ) +
    geom_bar()+
    scale_fill_manual(values=type_colors) +
    labs(subtitle =  "No Fraud Cases", x = "", y = "Frequency" ) + coord_flip() + theme_linedraw() +
    theme(axis.text.y = element_blank())

grid.arrange(p1,p2,nrow = 2)
```

Interestingly enough, frauds coincide only with Cash Out and Transfer transactions. That means the rest of the transaction types (Cash in, Debit, Payment) are never associated with fraud. We will get rid of all observations with these transactions, since they only lead to "Not Fraud".

Before we continue, let's look at how many rows we exterminated by cleaning the name and type columns.

```{r reduction,echo=FALSE}
rows_before <- dim(df)[1]

df <- df %>% filter( (!(type %in% c("CASH_IN", "DEBIT", "PAYMENT") )) , #remove transaction types
                   dest_name_prefix == "C") %>% #remove rows with M prefixes
            select(-dest_name_prefix) #drop the prefixes column

df$type <- (radiant.data::refactor( df$type, unique(df$type) ))

rows_after <- dim(df)[1]

diff <- rows_before - rows_after

diff_perc <- round(diff/rows_before * 100, 2)

sprintf( "%i rows removed, equivalent to %s%%  of the data", diff, diff_perc ) %>% print()
```

More than half of the initial rows can be thrown away. Good news for us!


And now let's visualize the transactions column before and after reduction.


```{r transaction_before_afte,echo=FALSE}
p11 <-  df %>% filter(isFraud == "Fraud") %>% 
    ggplot( aes( x = isFraud, fill = type ) ) +
    geom_bar()+
    scale_fill_manual(values=type_colors) +
    labs(title = "Transaction Type", subtitle =  "Fraud Cases", x = "", y = "" ) + coord_flip() + theme_linedraw()+
    theme(axis.text.y = element_blank())  

p12 <-df %>% filter(isFraud == "No_Fraud") %>% 
    ggplot( aes( x = isFraud, fill = type ) ) +
    geom_bar()+
    scale_fill_manual(values=type_colors) +
    labs(subtitle =  "No Fraud Cases", x = "", y = "Frequency" ) + coord_flip() + theme_linedraw()+
    theme(axis.text.y = element_blank())


grid.arrange(p1,p2,top = "Before Data Reduction")
grid.rect(gp = gpar(lwd = 3, col = "black", fill = NA))
grid.arrange(p11,p12,top = "After Data Reduction")
grid.rect(gp = gpar(lwd = 3, col = "black", fill = NA))
rm(p1,p2,p11,p12)
```

Looks like the no fraud cases contain more cash out transactions, while in frauds transfers are equally present. This might be a hint of the predictive power of this specific feature.

### Across Time

Even though it might seem irrational that frauds happen on a certain time instance, we can plot amount vs time coloured by isFraud column in case there is any obvious pattern.

Because observations are so many, they cover one another and our plot becomes uninformative. For that, we will randomly sample 10% of our data. This way we reduce the points plotted while avoiding biases.


```{r across_time_plot,echo=FALSE }
temp <- seq(0, max(as.numeric(df$step)) , 5 )

df %>% sample_n(floor(nrow(df)*.03)) %>% 
    ggplot()+
    geom_jitter(aes( x = as.numeric(step), y = log(amount+1,10)), alpha = .9, col = "#2ff7b5")+
    facet_grid(~isFraud)+
    scale_x_continuous(breaks = temp )+
    labs(x = "Hour", y = "Amount (in a log10 scale)", title = "Transaction Amounts by hour", subtitle = "")+
    theme_minimal()+
    theme(panel.border = element_rect(colour = "black", fill=NA, size=3) )
```

Apparently some hours contain little to no data. This could be due to some special circumstance (a strike or a power cut ) or just because the simulator didn't happen to produce enough data along these hours.

Fraud and Not Fraud cases look randomly scattered throughout time and do not seem to form clusters. However, there are some slight concentrations of frauds in some hours. This could be due to the fact that when fraudsters manage to commit fraud they try to maximize the number of transactions in the tight time window they have.

## Numerics

Now that we explored the character columns, let's look at our numerics.

### Amount

Since our numerics range from petty to enormous amounts, we will plot them after we apply the log10 function on them. This way, the distance between low and high amounts gets minimized and it becomes easier for the human eye to detect patterns.


```{r amount_violinplot,echo=FALSE}
df %>% 
    ggplot(aes(x = "isFraud", y = log(amount,10) , fill = isFraud)) + 
    geom_violin() + coord_flip() + 
    scale_fill_manual(values=c("#40448f", "#7d1f6d") , name="Mean", labels=c( "No Fraud","Fraud"))+
    labs(x = "", y = "Amount (in a log 10 scale)", title = "Amount of Transactions", subtitle = "Fraud vs Not Fraud Cases" ) +
    stat_summary(fun.y = "mean",geom = "point", size = 2, col =c("#40448f", "#7d1f6d"))+
    theme_classic() 
```

Oddly enough, the amount of transaction looks different in the fraud vs not fraud cases. No_fraud amounts range around 100,000 (10^5) while fraud cases are more evenly distributed. This could be a hint that the amount column can make a contribution in our model.

### Balance Columns

In contrast with the amount column, the *Balance* columns contain zero values. Zeros return a -Infinity when log is applied on them. 

A convenient solution to this problem is to add 1 on our numeric vector before we pass it to the log function. This way 0s become 1s and yield a 0 after the function is applied. This happens because whichever number raised to the power of 0 returns 1.

#### Balance of Origin

We start with the origin balances.

```{r  amount_boxplot,echo=FALSE}
which(df %>% sapply(is.numeric))[-1] %>% names() -> labs # store names for use in plot labels

p5 <- df %>%    
    ggplot(aes(x = isFraud, y = log(Old_Balance_of_Origin+1,10) , fill = isFraud)) + 
    geom_boxplot(alpha = .3) + coord_flip() + 
    scale_fill_manual(values=c("navy", "darkred"))+
    labs(x = "", y = paste(labs[1], "(in a log 10 scale)"), title = paste("Amount of", labs[1] ), subtitle = "Fraud vs Not Fraud Cases"  ) +
    theme_linedraw()


p6 <- df %>%
    ggplot(aes(x = isFraud, y = log(New_Balance_of_Origin+1,10) , fill = isFraud)) + 
    geom_boxplot(alpha = .3) + coord_flip() + 
    scale_fill_manual(values=c("navy", "darkred"))+
    labs(x = "", y = paste(labs[2], "(in a log 10 scale)"), title = paste("Amount of", labs[2] ) ) +
    theme_linedraw()

grid.arrange(p5,p6)
rm(p5,p6)
```

We observe a very high concentration of 0 values that render our plots completely uninformative. Let's try to omit them.


```{r amount_boxplot2, echo=FALSE}
p7 <- df %>%  filter(Old_Balance_of_Origin != 0) %>%
    ggplot(aes(x = isFraud, y = log(Old_Balance_of_Origin+1,10) , fill = isFraud)) + 
    geom_boxplot(alpha = .3) + coord_flip() + 
    scale_fill_manual(values=c("navy", "darkred"))+
    labs(x = "", y = paste(labs[1], "(in a log 10 scale)"), title = paste("Amount of", labs[1] ), subtitle = "Fraud vs Not Fraud Cases (zero values omitted)"  )+
    theme_linedraw()


p8 <- df %>% filter(New_Balance_of_Origin != 0) %>%
    ggplot(aes(x = isFraud, y = log(New_Balance_of_Origin+1,10) , fill = isFraud)) + 
    geom_boxplot(alpha = .3) + coord_flip() + 
    scale_fill_manual(values=c("navy", "darkred"))+
    labs(x = "", y = paste(labs[2], "(log 10 scale)"), title = paste("Amount of", labs[2] )  )+
    theme_linedraw()

grid.arrange(p7,p8)
rm(p7,p8)
```

A lot better.

Both in *Old* and in *New Balance of Origin*, frauds tend to happen in greater amounts of balances. 
This could also be a sign of predictive power.

#### Balance of Destination

We will follow the same conventions for **Balances of Destination**.


```{r destination_boxplot, echo=FALSE}
p9 <- df %>% filter(Old_Balance_of_Destination != 0) %>%
    ggplot(aes(x = isFraud, y = log(Old_Balance_of_Destination+1,10) , fill = isFraud)) + 
    geom_boxplot(alpha = .3) + coord_flip() + 
    scale_fill_manual(values=c("navy", "darkred"))+
    labs(x = "", y = paste(labs[3], "(in a log 10 scale)"), title = paste("Amount of", labs[3] ), subtitle = "Fraud vs Not Fraud Cases (zero values omitted)"  )+
    theme_linedraw()

p10 <- df %>% filter(New_Balance_of_Destination != 0) %>%
    ggplot(aes(x = isFraud, y = log(New_Balance_of_Destination+1,10) , fill = isFraud)) + 
    geom_boxplot(alpha = .3) + coord_flip() + 
    scale_fill_manual(values=c("navy", "darkred"))+
    labs(x = "", y = paste(labs[4], "(in a log 10 scale)"), title = paste("Amount of", labs[4] ) )+
    theme_linedraw()

grid.arrange(p9,p10)
rm(p9,p10)
```

In contrast with the origin balances, destination balances do not differ much between fraud and not_fraud cases.

# Feature Engineering

## The integer effect
 
Effective fraudsters might think that transfering a not round amount (eg 12,325 instead of 10,000) will seem like a more ordinary transaction and won't raise any suspicions. But on the other hand, they might not get into that trouble. Moreover, the human mind always comes up with rounded numbers and that would be the first thought of every (not so professional) fraudster.
That's why we will check whether round amounts of transactions coincide with fraud.


```{r isround_creation}
#Create a new isRound column 
df <- mutate(df, isRound = 
               as.factor(
                 ifelse( test = (df$amount - as.integer(df$amount)) == 0 , 
                         yes = 'round', no = 'float' )))
```


```{r isround_bar,echo=FALSE }
p11 <- df %>% filter( isFraud == "Fraud" ) %>% ggplot() + 
    geom_bar(aes( x = isFraud , fill = isRound ) ) +
    scale_fill_manual(values = c( "#849468", "#ded718" )) + 
    labs( title = "Round amounts of transactions" , x= "", y = "Count", subtitle = "Fraud Cases" )+
    theme_linedraw()

p12 <- df %>% filter( isFraud == "No_Fraud" ) %>% ggplot() + 
    geom_bar(aes( x = isFraud , fill = isRound ) ) +
    scale_fill_manual(values = c( "#849468", "#f7f300" ))+
    labs( x= "", y = "Count", subtitle = "No_Fraud Cases" ) +
    theme_linedraw()

grid.arrange(p11,p12)
rm(p11,p12)
```

Interestingly enough, round numbers seem to be more prevalent in fraud occurences. That's probably a clue of a valuable feature. (Note that in the no fraud cases I used a brighter yellow because otherwise it wouldn't be distinguishable)

## Is Balance Zero?

Another assumption one could make is that fraudsters use dummy accounts to send or (mostly) receive money. This is why it would be interesting to explore whether accounts with 0 balances are connected with fraud. 
For that, we will create a logical vector for each Balance column we already have .


```{r isZero_creation}
iszero <- df %>% select(-amount) %>% #exclude the amount that cannot be 0
    select_if(is.numeric) %>% #select the remaining numerics - ie the balances
    mutate_all(R.utils::isZero) %>% #check whether they are zero
    rename_all(~paste0('isZero_', substr(., 1, nchar(.) - 4))) #add an isZero prefix on the names

df <- df %>% cbind(iszero) #merge with original df
rm(iszero)
```

### Origin

First, we will take a look at the origin accounts.


```{r iszero_origin,echo=FALSE}
p13 <- df %>% filter(isFraud == "No_Fraud") %>%
    ggplot( aes( x = isFraud, fill = isZero_Old_Balance_of_Or ) ) + 
    geom_bar(position = "fill") + 
    labs(x = "", y = "", subtitle = "No Fraud") +
    geom_label(stat="count",aes(label= paste0(round(((..count..)/sum(..count..)*100),2),"%") ), position = position_fill(vjust=0.5)) +
    scale_fill_manual(values =  c("#69b0b3", "#d0e872"), #colors
                      labels = c("Not_Zero", "Zero"), #legend labels
                      name = "Is Old Balance of Origin Zero?") + #legend title
    theme_minimal()+ 
    coord_flip()+
    theme(axis.text.y = element_blank())


p14 <- df %>% filter(isFraud == "Fraud") %>%
    ggplot( aes( x = isFraud, fill = isZero_Old_Balance_of_Or ) ) + 
    geom_bar(position = "fill") + 
    labs(x = "", y = "Percentage",subtitle = "Fraud") +
    geom_label(stat="count",aes(label= paste0(round(((..count..)/sum(..count..)*100),2),"%") ), position = position_fill(vjust=0.5)) +
    scale_fill_manual(values =  c("#69b0b3", "#d0e872"), #colors
                      labels = c("Not_Zero", "Zero"), #legend labels
                      name = "Is Old Balance of Origin Zero?")+ #legend title
    theme_minimal()+
    coord_flip()+
    theme(axis.text.y = element_blank())

p15 <- df %>% filter(isFraud == "No_Fraud") %>%
    ggplot( aes( x = isFraud, fill = isZero_New_Balance_of_Or ) ) + 
    geom_bar(position = "fill") + 
    labs(x = "", y = "",subtitle="No Fraud") +
    geom_label(stat="count",aes(label= paste0(round(((..count..)/sum(..count..)*100),2),"%") ), position = position_fill(vjust=0.5)) +
    scale_fill_manual(values =  c("#69b0b3", "#d0e872"), 
                      labels = c("Not_Zero", "Zero"),
                      name = "Is New Balance of Origin Zero?") +
    theme_minimal() +
    coord_flip()+
    theme(axis.text.y = element_blank())

p16 <- df %>% filter(isFraud == "Fraud") %>%
    ggplot( aes( x = isFraud, fill = isZero_New_Balance_of_Or ) ) + 
    geom_bar(position = "fill") + 
    labs(x = "", y = "Percentage",subtitle = "Fraud") + 
    geom_label(stat="count",aes(label= paste0(round(((..count..)/sum(..count..)*100),2),"%") ), position = position_fill(vjust=0.5)) +
    scale_fill_manual(values =  c("#69b0b3", "#d0e872"), 
                      labels = c("Not_Zero", "Zero"),
                      name = "Is New Balance of Origin Zero?")+
    theme_minimal() + coord_flip()+
    theme(axis.text.y = element_blank())

grid.arrange(p13,p14,nrow = 2, top = "Old Balance of Origin")
grid.rect(gp = gpar(lwd = 3, col = "black", fill = NA))
grid.arrange(p15,p16,nrow = 2, top = "New Balance of Origin")
grid.rect(gp = gpar(lwd = 3, col = "black", fill = NA))

rm(p13,p14,p15,p16)
```

In Old Balance of Origin, fraud cases very seldom contain zero balances. That's expected because there's no point for a fraudster to target a zero balance account. 

In New Balance of Origin, on the other hand, 98% of the fraud cases leave it with zero balance (in other words, fraudsters loot the accounts to zero). That could prove zero New Balances of Origin as valuable predictors. 

### Destination

Now time for the Balances of Destination.


```{r destination_iszero,echo=FALSE }
p17 <- df %>% filter(isFraud == "No_Fraud") %>%
    ggplot( aes( x = isFraud, fill = isZero_Old_Balance_of_Destina ) ) + 
    geom_bar(position = "fill") + 
    labs(x = "", y = "Percentage",subtitle = "No Fraud") +
    geom_label(stat="count",aes(label= paste0(round(((..count..)/sum(..count..)*100),2),"%") ), position = position_fill(vjust=0.5)) +
    scale_fill_manual(values =  c("#69b0b3", "#d0e872"), #colors
                      labels = c("Not_Zero", "Zero"), #legend labels
                      name = "Is Old Balance of Destination Zero?") + #legend title
    theme_minimal()+ 
    coord_flip()+
    theme(axis.text.y = element_blank())

p18 <- df %>% filter(isFraud == "Fraud") %>%
    ggplot( aes( x = isFraud, fill = isZero_Old_Balance_of_Destina ) ) + 
    geom_bar(position = "fill") + 
    labs(x = "", y = "Percentage", subtitle = "Fraud") +
    geom_label(stat="count",aes(label= paste0(round(((..count..)/sum(..count..)*100),2),"%") ), position = position_fill(vjust=0.5)) +
    scale_fill_manual(values =  c("#69b0b3", "#d0e872"), #colors
                      labels = c("Not_Zero", "Zero"), #legend labels
                      name = "Is Old Balance of Destination Zero?")+ #legend title
    theme_minimal()+
    coord_flip()+
    theme(axis.text.y = element_blank())

p19 <- df %>% filter(isFraud == "No_Fraud") %>%
    ggplot( aes( x = isFraud, fill = isZero_New_Balance_of_Destina ) ) + 
    geom_bar(position = "fill") + 
    labs(x = "", y = "Percentage",subtitle = "No Fraud") +
    geom_label(stat="count",aes(label= paste0(round(((..count..)/sum(..count..)*100),2),"%") ), position = position_fill(vjust=0.5)) +
    scale_fill_manual(values =  c("#69b0b3", "#d0e872"), 
                      labels = c("Not_Zero", "Zero"),
                      name = "Is New Balance of Destination Zero?") +
    theme_minimal() +
    coord_flip()+
    theme(axis.text.y = element_blank())

p20 <- df %>% filter(isFraud == "Fraud") %>%
    ggplot( aes( x = isFraud, fill = isZero_New_Balance_of_Destina ) ) + 
    geom_bar(position = "fill") + 
    labs(x = "", y = "Percentage", subtitle = "Fraud") + 
    geom_label(stat="count",aes(label= paste0(round(((..count..)/sum(..count..)*100),2),"%") ), position = position_fill(vjust=0.5)) +
    scale_fill_manual(values =  c("#69b0b3", "#d0e872"), 
                      labels = c("Not_Zero", "Zero"),
                      name = "Is New Balance of Destination Zero?")+
    theme_minimal() + coord_flip()+
    theme(axis.text.y = element_blank())


grid.arrange(p17,p18,nrow = 2, top = "Old Balance of Destination") 
grid.rect(gp = gpar(lwd = 3, col = "black", fill = NA))
grid.arrange(p19,p20,nrow = 2, top = "New Balance of Destination") 
grid.rect(gp = gpar(lwd = 3, col = "black", fill = NA))
rm(p17,p18,p19,p20)
```

Both in Old and in New Balance of Destination, fraud cases coincide with zero balances way more frequently than no_fraud cases do. This can be easily explained by the fact that fraudsters usually send money to empty dummy accounts.

# Correlation Among Features

Before we start preprocessing, we will check for correlation between our numeric features with the help of a pairplot. Because the pairplot is very computationally expensive, we will choose 10% of the dataset for plotting.


```{r pairplot,echo=FALSE,warning=FALSE,message=FALSE}
#labels for plotting
labs <- c('Amount',"Old/Origin", "New/Origin", "Old/Destination","New/Destination") 

#Create a smaller dataset
df_small <- df %>% sample_n(floor(nrow(df)*.1))

#Pairplot
df_small[ ,sapply(df,is.numeric)]  %>% 
    ggpairs(mapping=ggplot2::aes(colour = df_small$isFraud), title = "Numeric Features",columnLabels = labs, axisLabels = "none")
rm(df_small)
```

Two very high correlations lie inside our numeric data. It appears that new and old balances (regarding both the origin and the destination) correlate. 
To decide on which columns to keep, we will utilize the findCorrelation function from caret package.

* Its first argument is going to be the **correlation table** of the numeric features of df. 

* By choosing a **cutoff** (border of absolute correlation) of 0.8 means that for each correlation below -0.8 or above 0.8, we get a suggestion of a column to eliminate.

* names = TRUE will **assign a character vector** to our newly created variable, making it easy to use with the select() function.

Additionaly, we notice a mild concentration of fraud cases near zero values for some of the Balance Columns. This fortifies our assumption that zero balances might coincide with fraud.


```{r high_cor_feats, verbose = FALSE}
high_cor_feats <- findCorrelation(
                  cor(df[ ,sapply(df,is.numeric)]), #compute corellation table
                  cutoff = .8, verbose = TRUE, # anything above .8 as high_cor
                  names = TRUE, exact = TRUE) #return the names of columns

# Show which features did not make it 
print("High Correlation Features to be Removed")
print(high_cor_feats)


# Drop the highly correlated columns
df <- df %>% select( -high_cor_feats )

```

# Preprocessing

Throughout the preprocessing section, we will create a more balanced dataset and perform train/test split on both the balanced and the unbalanced data.

## UnderSampling

Our goal is to create a more balance dataset. 

In my opinion, dropping the majority class prevalence to 50% totally eliminates the informational value of this imbalance. This is why in this kernel we will create an 60%-40% target variable.

This can be achieved by sampling from the majority class 6/4ths of the rows of minority instances. 
In an oversimplified example where the minority instances equal 40, the number of sampled majority instances will be 60 ( = 60/40 * 40), and the size of the new dataset totals 100.


```{r undersampling,echo=FALSE}
#Finding the number of fraud instances
n_minority <- df %>% filter(isFraud == "Fraud") %>% nrow()

#Sample the appropriate number of rows
rand_majority <- df %>% filter(isFraud == "No_Fraud") %>% sample_n( n_minority*6/4)

#Merge into a new dataset and arrange by time so that they get shuffled
df_bal <- df %>% filter(isFraud == "Fraud") %>% rbind( rand_majority ) %>% arrange(step)

#Repeat the frequencies bar plots to contrast with the earlier version of our dataset
p5 <- df %>% ggplot(aes(x = isFraud, y = (..count..))) + 
    geom_bar(fill =  c( "#5b7fb0", "#c2695b" ), stat = "count")+
    geom_label(stat='count',aes(   label=  paste0(  round(   ((..count..)/sum(..count..)) ,4)*100 ,  "%" ) ) )+
    labs(x = "Fraud or Not", y = "Frequency", title = "Frequency of Fraud before Undersampling", subtitle = "Labels as percent of Total Observations")+
    theme_linedraw()

p6 <- df_bal %>% ggplot(aes(x = isFraud, y = (..count..))) + 
    geom_bar(fill =  c( "#5b7fb0", "#c2695b" ), stat = "count")+
    geom_label(stat='count',aes(   label=  paste0(  round(   ((..count..)/sum(..count..)) ,4)*100 ,  "%" ) ) )+
    labs(x = "Fraud or Not", y = "Frequency", title = "Frequency of Fraud after Undersampling ", subtitle = "Labels as percent of Total Observations") + 
    theme_linedraw()

grid.arrange(p5,p6)
```

## Train/ Test Split

After we split our data into train and test set, we will create a preprocess routine based on the train data (test set is considered unseen). Our preprocess tranformation will include:

* centering , (ie subtracting by each column its own mean)
* scaling , (ie dividing by the column variance)

Then, we will use it to transform both sets.

The same process will be applied both on the undersampled as well as on the original dataset. Because we need to evaluate our algorithm based on real-life and not on synthetic, balanced data we will create train and test set of both categories.


```{r original_data_split, echo = FALSE}
#Create row indices for test/train split
indx <- createDataPartition(y = df$isFraud, p = .75, list = FALSE) 
train_set <- df[indx,] 
test_set <- df[-indx,]

#Train our preprocess function parameters
pp <- preProcess( train_set[, sapply(train_set,is.numeric) ] , 
                            method = c("center", "scale"))

#Transform the train_set
train_set <- predict(pp, newdata = train_set)

#Subsequently, transform the test_set
test_set <- predict(pp, newdata = test_set)

print( "Original (Unbalanced) Train Set Structure : " )
train_set %>% str() %>% print()

```


```{r unbal_data_split, echo=FALSE}
indx <- createDataPartition(y = df_bal$isFraud, p = .75, list = FALSE) #75% of data as train set 
train_set_b <- df_bal[indx,]
test_set_b <- df_bal[-indx,]

#Train our preprocess function parameters
pp_b <- preProcess( train_set_b[, sapply(train_set_b,is.numeric) ] , 
                            method = c("center", "scale"))

#Transform the train_set
train_set_b <- predict(pp_b, newdata = train_set_b)

#Subsequently, transform the test_set
test_set_b <- predict(pp_b, newdata = test_set_b)

print( "Balanced Train Set Structure : " )

train_set %>% str() %>% print()
```


# Modelling

The time has come to start modelling. Throughtout this section, we will compare performances for:

* train vs test set (both for tree and forest)

* balanced vs unbalanced set (both for tree and forest)

* and finally, Tree vs Forest (on unbalanced test set)

But first a quick glance on the cross-validation routine we are about to use.

## Cross Validation

To find the optimal model parameters (both for the tree and the forest), we will use cross-validation to train it. That means each time (total times are the repeats parameter) we train our algorithm, we split our train data into partitions (the number parameter), use one of them, and then evaluate its success on predicting the rest of the partitions. In our case, we measure predictions by summarizing each class probabilities with the twoClassSummary. This function fits well as the choices of prediction (the target variable's unique values) are two, Fraud or No_Fraud.


```{r traincontrol }
#Setting up the training parameters
tr <- trainControl(method = "repeatedcv", #repeated cross validation 
                        number = 9, #9 folds
                        repeats = 3, #3 times
                        classProbs = TRUE, #calculate probabilities for each class
                        summaryFunction = twoClassSummary) #use twoClassSummary to summarize probabilities
```

## Classification Tree

Before we train our tree, we need to decide on which parameters we want to optimize. Parameters are as follows:

*metric* - How we measure success of our algorithm. In every classification problem, we want as few False predictions as possible. But false predictions can be either False Positives (cases where we falsely predicted True) or False Negatives (where we falsely predicted False). These two metrics are inversely proportional. To make this statement more straightforward, imagine an algorithm that always predicts True (False). It will yield 0 False Negatives (Positives) and many False Positives (Negatives). Depending on the target, one can aim to minimize one of these errors. A useful tool to describe the balance between these errors is the ROC (Receiver Operating Characteristics) curve. 
To construct it, we use the False Positive Rate (FPR for short) for the X axis and the True Positive Rate (TPR for short) for the Y axis. TPR, also known as Recall or Sensitivity is calculated by dividing the True Positives by (True Positives + False Negatives). FPR, or (1 - Specificity), on the other hand, is False Positives divided by (False Positives + True Negatives). 
In case we predict randomly, our ROC curve will be a straight 45 degrees line starting from (0,0) because TPR equals FPR. Any improvement from that state will force the straight line to become a curve facing to the upper left corner (where TPR > FPR). As a result, the more area we have under the curve, the better is our algorithm in predicting the classes correctly. An ideal curve would occupy 100% of the available area. That's why we will use the Area Under the Curve to measure success of classification.

 *tuneLength* - The number of different default values used to optimize the main parameter of the model. In our case, that's the Complexity Parameter (CP). This parameter specifies how the cost of a tree C(T) is penalized by the number of terminal nodes. Because it penalizes, small (large) values of CP lead to large (small) trees with many (few) nodes that overfit (underfit). tuneLength defines how many different CPs we will use. For each CP we calculate our metric and finally pick the CP that yields the optimal metric value (area under the curve in our case). An alternative to 
 
 *split* - Criterion that shows which feature to split on at each step in building the tree. An alternative to it is Gini Impurity. Information Split it the most commonly used of the two. 


```{r tree_params}
#Model Training - Regression Tree
start_time <- Sys.time()

rpart_model_b <- train(isFraud ~ ., #training formula -- 
                                    #we want to predict isFraud by using all available predictors
                    data = train_set_b, #our balanced dataset
                    method = "rpart", # library containing the tree algorithm
                    tuneLength = 10, # number of different default values used to optimize complexity parameter
                    metric = "ROC", # estimate success by the area under the ROC curve
                    trControl = tr, # use cross validation as defined above
                    parms=list(split='information')) 

end_time <- Sys.time()

end_time - start_time

```

### Results on Balanced Data

Now that we trained our tree, let's see how it performs.

#### Train Set Results

First, the confusion matrix of the train set results

```{r train_set_results, echo = FALSE }
rpart_train_pred_b <- predict(rpart_model_b, train_set_b) # class predictions on train set

confusionMatrix(train_set_b$isFraud, rpart_train_pred_b) # report results of predictions
rm(rpart_train_pred_b)
```


#### Test Set Results

Then, the confusion matrix test set results.

```{r Test-Set-Results, echo=FALSE}
rpart_test_pred_b <- predict(rpart_model_b, test_set_b) #class predictions on test set
confusionMatrix(test_set_b$isFraud, rpart_test_pred_b) #results report
rm(rpart_test_pred_b)
```
#### ROC Curve - Comparing Train and Test Set Results

To visualize performance on balanced data, we will plot two ROC curves, one for the train and one for the test set. This plot will informa us not only about the raw performance, but also about the degree of overfitting. If the two lines are way apart from each other, our model overfits, meaning that it's good only in the specific data that has seen and cannot generalize.

```{r roc_test_set_b, echo = FALSE, warning=FALSE,message=FALSE}
rpart_probs_b <- predict(rpart_model_b, test_set_b, type = "prob") #probabilities predictions on balanced test set

rpart_ROC_b <- roc(response = test_set_b$isFraud, 
                 predictor = rpart_probs_b$Fraud, 
                 levels = levels(test_set_b$isFraud))

rpart_probs_b_train <- predict(rpart_model_b, train_set_b, type = "prob") #probabilities predictions on balanced test set

rpart_ROC_b_train <- roc(response = train_set_b$isFraud, 
                 predictor = rpart_probs_b_train$Fraud, 
                 levels = levels(train_set_b$isFraud))

plot(rpart_ROC_b, col = "#040491", main = "ROC Curve on Balanced Data Set (Tree Train vs Test)" )
plot(rpart_ROC_b_train, col = "green", add = TRUE)
legend(.4,.18, legend = c("Test Set", "Train Set"), col = c("#040491","green"), lty = 1 )
anot <- paste( "Train - Test Area : ", round(rpart_ROC_b_train$auc - rpart_ROC_b$auc, 3) )
text(labels = anot, x = 0.2, y = .25 )

```

Almost identical!

### Results on Unbalanced Data

To evaluate our algorithm in real life circumstances, we will pick a random sample (slightly smaller for ease of computations) from the original test set and try to predict on it. 

```{r random_sample_of_original }
#Pick a random sample from the unbalanced test set
random_sample <- test_set %>% 
                  sample_n(nrow(test_set)/1.3) #size = 75% of the test set rows
```

And the respective ROC curve:

```{r ROC_unbalanced, echo = FALSE,message=FALSE }
rpart_probs_c <- predict(rpart_model_b, random_sample, type = "prob")

rpart_ROC_c <- roc(response = random_sample$isFraud, 
                 predictor = rpart_probs_c$Fraud, 
                 levels = levels(random_sample$isFraud))

plot(rpart_ROC_c, col = "#63a105", main = "ROC curve on Unbalanced Test Set")

anot <-   paste("Area Under the Curve : " , round(rpart_ROC_c$auc,3))

text(labels = anot , x = 0, y= .2)

```

### Comparing Performance on Balanced vs Unbalanced Data 

Now time for the ROC curves that represent test set results on Balanced vs the Unbalanced Dataset.

```{r balanced_vs_unbalanced,echo = FALSE,message = FALSE,warning=FALSE }

plot(rpart_ROC_b, col = "#040491", main = "Balanced vs Unbalanced Test Set (Classification Tree)" )
plot(rpart_ROC_c, col = "#63a105",add = TRUE)
legend(.4,.18, legend = c("Balanced", "Unbalanced"), col = c("#040491","#63a105"), lty = 1 )

anot <- paste( "Balanced - Unbalanced Area : ", round((rpart_ROC_b$auc - rpart_ROC_c$auc), 4) )

text(labels = anot, x = 0.2, y = .25 )

```

A very slight difference of ```r round((rpart_ROC_b$auc - rpart_ROC_c$auc), 4)``` for the tree.

## Random Forest

Random forests are collections of trees. Instead of relying on a tree to predict, we build several and return the mode (most frequent class) of all predictions. The greatest advantage over the tree is that we avoid overfitting. On the other hand, it is much more computationally expensive.

As with the tree we will use the same cross-validation parameters and measure success with ROC curve. 

This time, instead of tuneLength that plugs in default values for the model hyperparameters, we will use a TuneGrid where we have these params preset. These hyperparameters include:

* *mtry* - Number of predictors included in search for a node split. Everytime When forming each split, instead of considering all predictors, a different random set of variables is selected within which the best split point is chosen. This feature mostly contributes to speed of algorithm. We will try 40% and 60% of the predictors.
 
* *ntree* - Number of trees to include in our forest.


```{r forest_tune_grid, echo=FALSE }
#Define Hyperparameters
tune_grid <- expand.grid(.mtry = c((floor(dim(random_sample)[2] * .6)),  # 60% and of predictors
                                   (floor(dim(random_sample)[2] * .4))) , # 40% of predictors
            .ntree = seq(70, 130, by = 30)) # number of trees

print("Tune Grid:")
print(tune_grid)
```

### Training

As we did with the Tree, we will train our forest using the balanced dataset. Then, we will evaluate its success on the same random sample we generated above.


```{r forest_train }
#Model Training - Random Forest
start_time <- Sys.time()
rand_for_b <- train(isFraud ~ ., 
                  data = train_set_b, #train on the balanced set
                  method="rf", # use the random forest package
                  metric = "ROC", # elect best model by ROC curve
                  TuneGrid = tune_grid, # use custom grid for hyperparameters
                  trControl=tr) # #use cross validation

end_time <- Sys.time()
end_time - start_time
```

#### Error by Number of Trees

Now that we trained our random forest, let's look at the estimated error per number of trees. 

The black line shows the [Out-Of-Bag Error](https://en.wikipedia.org/wiki/Out-of-bag_error), while the coloured lines show misclassification errors (Fraud identified as No_Frauds in green, No_Frauds identified as Frauds in red).


```{r error_ntree, echo = FALSE}
#plot error versus number of trees
plot(rand_for_b$finalModel)
```


Seems like all lines reach a plateau at around 50 trees.

#### Variable Importance

Now that we've trained our random forest we can check out the variable importance to confirm whether the features we created contain any value.


```{r varimp, echo=FALSE }
#Variable Importance

imps <- rand_for_b$finalModel$importance

nams <- (imps %>% attr("dimnames"))[[1]]

imps <- imps %>% as.data.frame() %>% cbind(nams) %>% filter(MeanDecreaseGini > 1)

imps  %>%
  ggplot( aes(x = reorder(nams,MeanDecreaseGini), y = MeanDecreaseGini ) ) +
  geom_point(size = 9, col = "#ff583b")+
  geom_vline(xintercept = 1: nrow(imps), col = "darkgray") + 
  geom_text( aes(label = round(MeanDecreaseGini,2), size = 5), show.legend = FALSE , col = "black" ) +
  coord_flip()+
  labs(title = "Variable Importance", x = "", y = "",subtitle = "Features with Importance > 1" )+
  theme_classic()

#randomForest::varImpPlot(rand_for_b$finalModel)

```

As we assumed above, engineered features (isInteger, isZero) seem to play an important role in predicting the outcome.

### Train vs Test Set (Balanced Dataset)

As we did with the tree, we will take a look at the train vs test set performance:

```{r test_set_preds, echo=FALSE }
#Predictions on train set
rand_for_pred_b_train <- predict(rand_for_b, train_set_b) #class predictions on test set
print( "Train Set Confusion Matrix (Balanced Data) : " )
confusionMatrix(train_set_b$isFraud, rand_for_pred_b_train) #results report

#Predictions on test set
rand_for_pred_b <- predict(rand_for_b, test_set_b) #class predictions on test set
print( "Test Set Confusion Matrix (Balanced Data) : " )
confusionMatrix(test_set_b$isFraud, rand_for_pred_b) #results report

#probabilities predictions on balanced train set
rand_for_probs_b <- predict(rand_for_b, train_set_b, type = "prob") 

#respective ROC curve
rand_for_ROC_b <- roc(response = train_set_b$isFraud, 
                 predictor = rand_for_probs_b$Fraud, 
                 levels = levels(train_set_b$isFraud))

#probabilities predictions on balanced test set
rand_for_probs_b_test <- predict(rand_for_b, test_set_b, type = "prob") 

#respective ROC curve
rand_for_ROC_b_test <- roc(response = test_set_b$isFraud, 
                 predictor = rand_for_probs_b_test$Fraud, 
                 levels = levels(test_set_b$isFraud))


plot(rand_for_ROC_b, col = "#040491", main = "ROC Curve on Balanced Data Set (Forest Train vs Test)" )
plot(rand_for_ROC_b_test, col = "green", add = TRUE)
legend(.4,.18, legend = c("Train Set", "Test Set"), col = c("#040491","green"), lty = 1 )

```

### Balanced vs Unbalanced

```{r random_sample_preds, echo = FALSE }
rand_for_pred_c <- predict(rand_for_b, random_sample) #class predictions on random sample set
print("Rando Sample Confusion Matrix (Unbalanced Data) : ")
confusionMatrix(random_sample$isFraud, rand_for_pred_c) #results report
```


```{r random_ROC, echo=FALSE}
rand_for_probs_c <- predict(rand_for_b, random_sample, type = "prob") #probabilities predictions on random sample

rand_for_ROC_c <- roc(response = random_sample$isFraud, 
                 predictor = rand_for_probs_c$Fraud, 
                 levels = levels(random_sample$isFraud))

plot(rand_for_ROC_c, col = "#627502", main = "Balanced vs Unbalanced (Random Forest)" )
plot(rand_for_ROC_b_test, col = "green",add = TRUE)
legend(.4,.18, legend = c("Unbalanced", "Balanced"), col = c("#627502","green"), lty = 1 )
```


### Tree vs Forest

The time has come to compare performance between the two algorithms. For that, we will employ their ROC curves obtained by the unbalanced test sets.


```{r tree_vs_forest, echo = FALSE }
plot(rand_for_ROC_c, col = "#627502", main = "Balanced vs Unbalanced Test Set (Random Forest)" )
plot(rpart_ROC_c, col = "#63a105",add = TRUE)
legend(.4,.18, legend = c("Random Forest", "Classification Tree"), col = c( "#627502","#63a105"), lty = 1 )

print("Area Under The Curve for Balanced")
print(rand_for_ROC_c$auc)

print("Area Under The Curve for Unbalanced")
print(rpart_ROC_c$auc)

print("Difference between areas")
print(rand_for_ROC_c$auc - rpart_ROC_c$auc)
```

It becomes obvious that, at least for the AUC metric, **Forest outperforms the Tree**.

# Conclusion

Both classification trees and random forests can become efficient predictors of fraud, provided that they get their hyperparameters tuned and the underlying data gets engineered. 

By this kernel, results seem to slightly favor the random forest against the classification tree but the difference is so small that one could trade it off for speed of training (usually trees are quicker to train).

Interesting extensions of this kernel include: 

* comparing models trained with datasets of different rates of undersampling (mainly undersampled vs non-undersampled training).
* experiment with more metrics
* test more sophisticated techniques like [C5.0](https://topepo.github.io/C5.0/articles/C5.0.html), [Gradient Boosting](https://en.wikipedia.org/wiki/Gradient_boosting) or [Support Vector Machines](https://en.wikipedia.org/wiki/Support-vector_machine)
* create ensemble models (ie combinations of different machine learning algorithm)

# Appendix (Code)